{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 👩‍💻 Day 8: CLIP in Action — Fine-Tuning and Probing Multimodal Capabilities\n\nWelcome back to Day 8 of our VLM journey! 🎯 Yesterday, we dissected the powerful dual-encoder architecture and contrastive learning mechanism that makes OpenAI’s CLIP model so effective.\n\nToday, we’ll bring all that theory to life — by fine-tuning CLIP on a small image-text dataset and exploring its performance using zero-shot probing and classification tasks.\n\nLet’s dive in 🚀\n\n🛠️ What We'll Build\n* We'll set up a Kaggle-friendly experiment that does the following:\n* Loads a pretrained CLIP model.\n* Prepares a small image-text dataset (we’ll use Flickr8k or a custom dummy dataset if needed).\n* Encodes both image and text using CLIP encoders.\n* Trains a linear probing head (optional) or fine-tunes CLIP.\n* Applies early stopping to avoid overfitting.\n* Evaluates using cosine similarity.\n* Visualizes predictions and logs training results.\n\n","metadata":{}},{"cell_type":"markdown","source":"### ✅ Step 0: Setup (Kaggle Environment & Libraries)\n\nKaggle notebooks already have many libraries preinstalled, but you may need to install HuggingFace and torchvision manually.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers torchvision ftfy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:19:06.014045Z","iopub.execute_input":"2025-06-15T12:19:06.014293Z","iopub.status.idle":"2025-06-15T12:20:25.655111Z","shell.execute_reply.started":"2025-06-15T12:19:06.014270Z","shell.execute_reply":"2025-06-15T12:20:25.654150Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom transformers import CLIPProcessor, CLIPModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:22:38.124521Z","iopub.execute_input":"2025-06-15T12:22:38.125201Z","iopub.status.idle":"2025-06-15T12:22:55.488869Z","shell.execute_reply.started":"2025-06-15T12:22:38.125176Z","shell.execute_reply":"2025-06-15T12:22:55.488076Z"}},"outputs":[{"name":"stderr","text":"2025-06-15 12:22:43.992041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749990164.171440      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749990164.226619      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### 📦 Step 1: Load Pretrained CLIP Model from HuggingFace\n\nWe’ll use OpenAI’s CLIP ViT-B/32 variant — a widely adopted and efficient version of CLIP with a strong balance between performance and speed.","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:28:28.932476Z","iopub.execute_input":"2025-06-15T12:28:28.933437Z","iopub.status.idle":"2025-06-15T12:28:35.566092Z","shell.execute_reply.started":"2025-06-15T12:28:28.933412Z","shell.execute_reply":"2025-06-15T12:28:35.565510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"200317074242401bb0dc77aa9ff3ff4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf8975e64714d79822491efc49d86f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdce24bd1b27423ea517c22c995fd04f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4dc12a340ce4518b40d3081ad6d2b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540b70627a3c48da8fb483595ef78af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01578aa1f96a4082924898c918dbc24c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7490b850c04fc5aa4d8d1ca386d7c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52497152d50489da1e5859a8fdc3496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b677a27a37ec49929d4d13d945237475"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### 🖼️ Step 2: Prepare Your Dataset\n\nYou can upload the Flickr8k dataset.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport torchvision.transforms as transforms\n\nclass ImageTextDataset(Dataset):\n    def __init__(self, image_paths, captions, processor, transform=None):\n        self.image_paths = image_paths\n        self.captions = captions\n        self.processor = processor\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        caption = self.captions[idx]\n        return image, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:39:56.145206Z","iopub.execute_input":"2025-06-15T12:39:56.145517Z","iopub.status.idle":"2025-06-15T12:39:56.151359Z","shell.execute_reply.started":"2025-06-15T12:39:56.145495Z","shell.execute_reply":"2025-06-15T12:39:56.150657Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from collections import defaultdict\n\ncaption_dict = defaultdict(list)\n\nwith open(\"/kaggle/input/flickr8k/captions.txt\", \"r\") as f:  # Update the path\n    next(f)  # skip header\n    for line in f:\n        parts = line.strip().split(',', 1)  # Split only at the first comma\n        if len(parts) != 2:\n            continue  # Skip bad lines\n        filename, caption = parts\n        caption_dict[filename].append(caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:42:45.424879Z","iopub.execute_input":"2025-06-15T12:42:45.425535Z","iopub.status.idle":"2025-06-15T12:42:45.487390Z","shell.execute_reply.started":"2025-06-15T12:42:45.425515Z","shell.execute_reply":"2025-06-15T12:42:45.486886Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\n\nimage_folder = \"/kaggle/input/flickr8k/Images\"  # Path to your image folder\nimage_paths, captions = [], []\n\nfor img_name, caps in caption_dict.items():\n    full_path = os.path.join(image_folder, img_name)\n    if os.path.exists(full_path):\n        image_paths.append(full_path)\n        captions.append(caps[0])  # use the first caption only\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:43:03.893325Z","iopub.execute_input":"2025-06-15T12:43:03.893781Z","iopub.status.idle":"2025-06-15T12:43:16.919304Z","shell.execute_reply.started":"2025-06-15T12:43:03.893759Z","shell.execute_reply":"2025-06-15T12:43:16.918530Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(f\"Total Image-Caption Pairs: {len(image_paths)}\")\nprint(\"Sample Image Path:\", image_paths[0])\nprint(\"Sample Caption:\", captions[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:43:30.704439Z","iopub.execute_input":"2025-06-15T12:43:30.705113Z","iopub.status.idle":"2025-06-15T12:43:30.708999Z","shell.execute_reply.started":"2025-06-15T12:43:30.705087Z","shell.execute_reply":"2025-06-15T12:43:30.708319Z"}},"outputs":[{"name":"stdout","text":"Total Image-Caption Pairs: 8091\nSample Image Path: /kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg\nSample Caption: A child in a pink dress is climbing up a set of stairs in an entry way .\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 🔁 Step 3: Encode Images and Captions Using CLIP\n\nNow we’ll use CLIPProcessor to preprocess both images and text, and CLIPModel to generate image and text embeddings.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass CLIPDataset(Dataset):\n    def __init__(self, image_paths, captions):\n        self.image_paths = image_paths\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        caption = self.captions[idx]\n        return image, caption\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:01:46.217961Z","iopub.execute_input":"2025-06-15T13:01:46.218244Z","iopub.status.idle":"2025-06-15T13:01:46.222919Z","shell.execute_reply.started":"2025-06-15T13:01:46.218223Z","shell.execute_reply":"2025-06-15T13:01:46.222158Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def collate_fn(batch):\n    images, texts = zip(*batch)\n    inputs = processor(\n        text=list(texts),\n        images=list(images),\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True\n    )\n    return inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:02:03.617130Z","iopub.execute_input":"2025-06-15T13:02:03.617407Z","iopub.status.idle":"2025-06-15T13:02:03.621742Z","shell.execute_reply.started":"2025-06-15T13:02:03.617387Z","shell.execute_reply":"2025-06-15T13:02:03.621039Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### 🧪 Create Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"dataset = CLIPDataset(image_paths, captions)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:02:16.503199Z","iopub.execute_input":"2025-06-15T13:02:16.503479Z","iopub.status.idle":"2025-06-15T13:02:16.507522Z","shell.execute_reply.started":"2025-06-15T13:02:16.503459Z","shell.execute_reply":"2025-06-15T13:02:16.506871Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### 🔍 Quick Sanity Check","metadata":{}},{"cell_type":"code","source":"batch = next(iter(loader))\nfor key in batch:\n    print(f\"{key}: shape = {batch[key].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:02:26.829184Z","iopub.execute_input":"2025-06-15T13:02:26.829448Z","iopub.status.idle":"2025-06-15T13:02:26.886451Z","shell.execute_reply.started":"2025-06-15T13:02:26.829429Z","shell.execute_reply":"2025-06-15T13:02:26.885839Z"}},"outputs":[{"name":"stdout","text":"input_ids: shape = torch.Size([4, 16])\nattention_mask: shape = torch.Size([4, 16])\npixel_values: shape = torch.Size([4, 3, 224, 224])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"with torch.no_grad():\n    image_embeds = model.get_image_features(pixel_values=batch[\"pixel_values\"].to(device))\n    text_embeds = model.get_text_features(input_ids=batch[\"input_ids\"].to(device),\n                                          attention_mask=batch[\"attention_mask\"].to(device))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:02:38.369142Z","iopub.execute_input":"2025-06-15T13:02:38.369413Z","iopub.status.idle":"2025-06-15T13:02:38.790262Z","shell.execute_reply.started":"2025-06-15T13:02:38.369394Z","shell.execute_reply":"2025-06-15T13:02:38.789724Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### 🧠 Step 4: Training Loop with InfoNCE Loss (Contrastive Learning)\n\nWe'll now define the core components to train/fine-tune CLIP using image-text pairs.","metadata":{}},{"cell_type":"code","source":"# Define Cosine Similarity & Contrastive Loss (InfoNCE)\nimport torch.nn.functional as F\n\ndef clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n    # Normalize embeddings\n    image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n\n    # Compute cosine similarity matrix\n    logits_per_image = torch.matmul(image_embeds, text_embeds.T) / temperature\n    logits_per_text = logits_per_image.T\n\n    batch_size = image_embeds.size(0)\n    labels = torch.arange(batch_size).to(image_embeds.device)\n\n    loss_i = F.cross_entropy(logits_per_image, labels)\n    loss_t = F.cross_entropy(logits_per_text, labels)\n\n    return (loss_i + loss_t) / 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:04:16.003504Z","iopub.execute_input":"2025-06-15T13:04:16.004154Z","iopub.status.idle":"2025-06-15T13:04:16.009502Z","shell.execute_reply.started":"2025-06-15T13:04:16.004132Z","shell.execute_reply":"2025-06-15T13:04:16.008739Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Set Up Optimizer & Model\nfrom transformers import CLIPModel\nimport torch\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:04:40.694738Z","iopub.execute_input":"2025-06-15T13:04:40.695016Z","iopub.status.idle":"2025-06-15T13:04:41.741621Z","shell.execute_reply.started":"2025-06-15T13:04:40.694997Z","shell.execute_reply":"2025-06-15T13:04:41.741074Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Training Loop (Basic)\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in loader:\n        pixel_values = batch['pixel_values'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_loss=False\n        )\n\n        image_embeds = outputs.image_embeds\n        text_embeds = outputs.text_embeds\n\n        loss = clip_contrastive_loss(image_embeds, text_embeds)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:05:06.476529Z","iopub.execute_input":"2025-06-15T13:05:06.477186Z","iopub.status.idle":"2025-06-15T13:23:51.347169Z","shell.execute_reply.started":"2025-06-15T13:05:06.477162Z","shell.execute_reply":"2025-06-15T13:23:51.346363Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.1264\nEpoch 2/5, Loss: 0.0599\nEpoch 3/5, Loss: 0.0610\nEpoch 4/5, Loss: 0.0622\nEpoch 5/5, Loss: 0.0475\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"#### 📝 Bonus: Evaluation Code Snippet","metadata":{}},{"cell_type":"code","source":"\n\nmodel.eval()\nwith torch.no_grad():\n    batch = next(iter(loader))\n    outputs = model(\n        pixel_values=batch['pixel_values'].to(device),\n        input_ids=batch['input_ids'].to(device),\n        attention_mask=batch['attention_mask'].to(device),\n        return_loss=False\n    )\n\n    sim_matrix = torch.matmul(\n        F.normalize(outputs.image_embeds, dim=1),\n        F.normalize(outputs.text_embeds, dim=1).T\n    )\n\n    print(\"Similarity Matrix:\\n\", sim_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:49:52.460619Z","iopub.execute_input":"2025-06-15T13:49:52.461119Z","iopub.status.idle":"2025-06-15T13:49:52.638287Z","shell.execute_reply.started":"2025-06-15T13:49:52.461093Z","shell.execute_reply":"2025-06-15T13:49:52.637527Z"}},"outputs":[{"name":"stdout","text":"Similarity Matrix:\n tensor([[ 0.3049, -0.1690, -0.0891,  0.1364],\n        [-0.1880,  0.5654, -0.0407, -0.1371],\n        [-0.1063, -0.1542,  0.5103, -0.0806],\n        [ 0.0287, -0.0455, -0.1565,  0.6267]], device='cuda:0')\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}